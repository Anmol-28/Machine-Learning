{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import keras\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D # to add convolutional layers\n",
    "from keras.layers.convolutional import MaxPooling2D # to add pooling layers\n",
    "from keras.layers.advanced_activations import LeakyReLU,ThresholdedReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    historydf.plot(ylim=(0, max(1, historydf.values.max())))\n",
    "    loss = history.history['val_loss'][-1]\n",
    "    acc = history.history['val_acc'][-1]\n",
    "    plt.title('Validation Loss: %.3f, Validation Accuracy: %.3f' % (loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categories = pd.read_csv('train.csv')\n",
    "train_images = train_categories.id.values.tolist()\n",
    "location = 'C:\\\\Users\\\\ak19919\\\\Downloads\\\\Github\\\\Analytics-Vidya\\\\Apparels'\n",
    "trainLabels = {}\n",
    "f = open(\"train.csv\", \"r\")\n",
    "clothes = f.read()\n",
    "clothes = clothes.split('\\n')\n",
    "\n",
    "for i in tqdm(range(len(clothes) - 1)):\n",
    "    clothes[i] = clothes[i].split(',')\n",
    "    trainLabels[clothes[i][0]] = clothes[i][1]\n",
    "del trainLabels['id']\n",
    "\n",
    "test_images = pd.read_csv('test.csv')\n",
    "testImages = test_images.id.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apparels = trainLabels.values()\n",
    "trainSet = set(Apparels)\n",
    "itr_set = {}\n",
    "for i in trainSet:\n",
    "    itr_set[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(location + str('\\\\train_labelled')):\n",
    "    os.makedirs(location + str('\\\\train_labelled'))\n",
    "    os.makedirs(location + str('\\\\test_labelled'))\n",
    "    \n",
    "    # Combine labels and images and move to labelled train folder\n",
    "    for img in tqdm(os.listdir(location + '\\\\train')):\n",
    "        if not int(img.split('.')[0]) in train_images:\n",
    "            continue\n",
    "        imgName = img.split('.')[0]\n",
    "        label = trainLabels[str(imgName)]\n",
    "        itr_set[label] += 1\n",
    "        path = os.path.join(location + '\\\\train\\\\', img)\n",
    "        saveName = location + '\\\\train_labelled\\\\' + label + '-' + str(itr_set[label]) + '.png'\n",
    "        image_data = np.array(Image.open(path))\n",
    "        imageio.imwrite(saveName, image_data)\n",
    "        \n",
    "    # Move 20% of labelled data to validation folder for testing\n",
    "    validation_data = os.listdir(location + '\\\\train_labelled')\n",
    "    random.Random(28).shuffle(validation_data)\n",
    "    for i in itr_set:\n",
    "        itr_set[i] = int(itr_set[i]*0.2)\n",
    "    for i in tqdm(itr_set):\n",
    "        for j in validation_data:\n",
    "            if j.split('-')[0] == i:\n",
    "                if itr_set[i] > 0:\n",
    "                    shutil.move(location + '\\\\train_labelled\\\\' + str(j), location + str('\\\\test_labelled'))\n",
    "                    itr_set[i] -= 1\n",
    "\n",
    "# Move unlabelled data for classification to test folder\n",
    "if not os.path.exists(location + str('\\\\test_images')):\n",
    "    os.makedirs(location + str('\\\\test_images'))\n",
    "    for image in tqdm(testImages):\n",
    "        shutil.move(location + '\\\\test\\\\' + str(image) + '.png', location + str('\\\\test_images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_img(name):\n",
    "    word_label = name.split('-')[0]\n",
    "    if word_label == '0' : return np.array([1,0,0,0,0,0,0,0,0,0])\n",
    "    elif word_label == '1' : return np.array([0,1,0,0,0,0,0,0,0,0])\n",
    "    elif word_label == '2' : return np.array([0,0,1,0,0,0,0,0,0,0])\n",
    "    elif word_label == '3' : return np.array([0,0,0,1,0,0,0,0,0,0])\n",
    "    elif word_label == '4' : return np.array([0,0,0,0,1,0,0,0,0,0])\n",
    "    elif word_label == '5' : return np.array([0,0,0,0,0,1,0,0,0,0])\n",
    "    elif word_label == '6' : return np.array([0,0,0,0,0,0,1,0,0,0])\n",
    "    elif word_label == '7' : return np.array([0,0,0,0,0,0,0,1,0,0])\n",
    "    elif word_label == '8' : return np.array([0,0,0,0,0,0,0,0,1,0])\n",
    "    elif word_label == '9' : return np.array([0,0,0,0,0,0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_statistics(DIR):\n",
    "    heights = []\n",
    "    widths = []\n",
    "    for img in tqdm(os.listdir(DIR)): \n",
    "        path = os.path.join(DIR, img)\n",
    "        data = np.array(Image.open(path)) #PIL Image library\n",
    "        heights.append(data.shape[0])\n",
    "        widths.append(data.shape[1])\n",
    "    avg_height = sum(heights) / len(heights)\n",
    "    avg_width = sum(widths) / len(widths)\n",
    "    print(\"Average Height: \" + str(avg_height))\n",
    "    print(\"Max Height: \" + str(max(heights)))\n",
    "    print(\"Min Height: \" + str(min(heights)))\n",
    "    print(\"Average Width: \" + str(avg_width))\n",
    "    print(\"Max Width: \" + str(max(widths)))\n",
    "    print(\"Min Width: \" + str(min(widths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_size_statistics(location + '\\\\train_labelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 48\n",
    "def load_training_data(DIR):\n",
    "    train_data = []\n",
    "    for img in tqdm(os.listdir(DIR)):\n",
    "        label = label_img(img)\n",
    "        path = os.path.join(DIR, img)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        train_data.append([np.array(img), label])\n",
    "    shuffle(train_data)\n",
    "    return train_data\n",
    "\n",
    "def load_validation_data(DIR):\n",
    "    val_data = []\n",
    "    for img in tqdm(os.listdir(DIR)):\n",
    "        label = label_img(img)\n",
    "        path = os.path.join(DIR, img)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        val_data.append([np.array(img), label])\n",
    "    shuffle(val_data)\n",
    "    return val_data\n",
    "\n",
    "def load_testing_data(DIR):\n",
    "    test_data = []\n",
    "    for Img in tqdm(os.listdir(DIR)):\n",
    "        path = os.path.join(DIR, Img)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        test_data.append([np.array(img), Img])\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_training_data(location + '\\\\train_labelled')\n",
    "val_data = load_validation_data(location + '\\\\test_labelled')\n",
    "X_train = np.array([i[0] for i in train_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "X_train = X_train / 255 # normalize training data\n",
    "y_train = np.array([i[1] for i in train_data])\n",
    "y_train = y_train / 255 # normalize training data\n",
    "X_test = np.array([i[0] for i in val_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "X_test = X_test / 255 # normalize test data\n",
    "y_test = np.array([i[1] for i in val_data])\n",
    "y_test = y_test / 255 # normalize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data[1247][0], cmap = 'gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convolutional_model():\n",
    "    ADAMAX = optimizers.Adamax(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999)\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu', kernel_initializer = 'he_uniform', input_shape = (IMG_SIZE, IMG_SIZE, 1)))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2), padding = 'same'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'linear', padding = 'same'))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2), padding = 'same'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(128, (3, 3), activation = 'linear',padding = 'same'))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2),padding = 'same'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation = 'linear'))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(Dense(10, activation = 'softmax'))\n",
    "    # Compile model\n",
    "    model.compile(optimizer = ADAMAX, loss = 'categorical_crossentropy',  metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range = 10, width_shift_range = 0.1, shear_range = 0.1,\n",
    "                         height_shift_range = 0.1, zoom_range = 0.25, fill_mode = 'nearest', horizontal_flip = True,\n",
    "                         vertical_flip = False, featurewise_center = False,\n",
    "                         samplewise_center = False, featurewise_std_normalization = False,\n",
    "                         samplewise_std_normalization = False)\n",
    "test_gen = ImageDataGenerator()\n",
    "\n",
    "# Create batches to  train models faster\n",
    "train_generator = gen.flow(X_train, y_train, batch_size = 256)\n",
    "test_generator = test_gen.flow(X_test, y_test, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use annelar to gradually decrese the learning rate to improve generalization\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_acc', patience = 20, verbose = 1, factor = 0.4, min_lr = 0.00002,\n",
    "                                            mode = 'auto', cooldown = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = convolutional_model()\n",
    "epochs = 100\n",
    "# fit the model\n",
    "\n",
    "history = model.fit_generator(train_generator, steps_per_epoch = 190, epochs = epochs, \n",
    "                              validation_data = test_generator, validation_steps = 48, verbose = 1,\n",
    "                              callbacks=[reduce_lr])\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose = 1)\n",
    "print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_testing_data(location + '\\\\test_images')\n",
    "test = np.array([i[0] for i in test_data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "test_labels = np.array([i[1] for i in test_data])\n",
    "test = test / 255 # normalize test data\n",
    "Y_pred = np.round(model.predict(test))\n",
    "Y_pred = np.argmax(Y_pred, axis = 1)\n",
    "Y_pred = pd.Series(Y_pred, name = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "                  \"id\": pd.Series(test_labels),\n",
    "                  \"label\": pd.Series(Y_pred)})\n",
    "submission_df['id'] = submission_df['id'].apply(lambda x: x.split('.')[0])\n",
    "submission_df.to_csv('submission_3_96p_3CLBMD_FDLD.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "model_name = 'classify_apparels-{}-{}.model'.format(LR, '96p_3CLBMD_FDLD_conv_v3')\n",
    "model.save(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
