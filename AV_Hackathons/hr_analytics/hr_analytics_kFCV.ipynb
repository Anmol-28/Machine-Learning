{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    historydf.plot(ylim=(0, max(1, historydf.values.max())))\n",
    "    loss = history.history['loss'][-1]\n",
    "    acc = history.history['acc'][-1]\n",
    "    plt.title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bin_loss_accuracy(history):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    historydf.plot(ylim=(0, max(1, historydf.values.max())))\n",
    "    loss = history.history['loss'][-1]\n",
    "    acc = history.history['binary_accuracy'][-1]\n",
    "    plt.title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/ak19919/Downloads/ml_root/analytics vidya/hr_analytics/train.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('employee_id')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values and adding info column\n",
    "\n",
    "for column in ['education', 'previous_year_rating']:\n",
    "    data[column].fillna(data[column].mode()[0], inplace = True)\n",
    "    \n",
    "data['joining_age'] = data['age'] - data['length_of_service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only numerical data fields\n",
    "df_num = data.select_dtypes(include = ['float64', 'int64'])\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_num.corr()[['is_promoted']], annot=True, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
    "sns.heatmap(df_num.corr(), annot=True, square=True, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As age is a categorical variable, instead of scaling this feature like other variables, Quantile based binning \n",
    "# is a good strategy to use for adaptive binning. Quantiles are specific values or cut-points which help in \n",
    "# partitioning the continuous valued distribution of a specific numeric field into discrete contiguous bins \n",
    "# or intervals. Thus, q-Quantiles help in partitioning a numeric attribute into q equal partitions\n",
    "\n",
    "quantile_list = [0, .25, .5, .75, 1.]\n",
    "age_quantiles = data['age'].quantile(quantile_list)\n",
    "age_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "data['age'].hist(bins=30, color='#A9C5D3', \n",
    "                             edgecolor='black', grid=False)\n",
    "for quantile in age_quantiles:\n",
    "    qvl = plt.axvline(quantile, color='r')\n",
    "ax.legend([qvl], ['Quantiles'], fontsize=10)\n",
    "ax.set_title('Age Histogram with Quantiles', fontsize=12)\n",
    "ax.set_xlabel('Age', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_labels = ['0-25Q', '25-50Q', '50-75Q', '75-100Q']\n",
    "# data['age_quantile_range'] = pd.qcut(data['age'], q=quantile_list)\n",
    "data['age_range'] = pd.qcut(data['age'], q=quantile_list, labels=quantile_labels)\n",
    "data = data.drop('age', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Join_age_quantiles = data['joining_age'].quantile(quantile_list)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data['joining_age'].hist(bins=30, color='#A9C5D3', \n",
    "                             edgecolor='black', grid=False)\n",
    "for quantile in Join_age_quantiles:\n",
    "    qvl = plt.axvline(quantile, color='r')\n",
    "ax.legend([qvl], ['Quantiles'], fontsize=10)\n",
    "ax.set_title('Joining Age Histogram with Quantiles', fontsize=12)\n",
    "ax.set_xlabel('Joining Age', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "data['join_age_range'] = pd.qcut(data['joining_age'], q=quantile_list, labels=quantile_labels)\n",
    "data = data.drop('joining_age', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOS = np.array(data['length_of_service'])\n",
    "## LOS_clean = income[~np.isnan(LOS)]\n",
    "#l, opt_lambda = stats.boxcox(LOS)\n",
    "#print('Optimal lambda value for Length of service:', opt_lambda)\n",
    "\n",
    "## data['rating_BC_0'] = stats.boxcox((1+data['previous_year_rating']), lmbda = 0)\n",
    "#data['LOS_BC_opt'] = stats.boxcox(data['length_of_service'], lmbda = opt_lambda)\n",
    "#data = data.drop('length_of_service', axis=1)\n",
    "#data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding categorical features\n",
    "\n",
    "cat_features = ['gender', 'education', 'recruitment_channel', 'region', 'department', 'age_range', 'join_age_range']\n",
    "df_cat = pd.get_dummies(data[cat_features])\n",
    "data = data.drop(cat_features, axis=1)\n",
    "data = pd.concat([data, df_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features within range (0, 1)\n",
    "\n",
    "ss = StandardScaler()\n",
    "scale_features = ['no_of_trainings', 'previous_year_rating', 'length_of_service', 'avg_training_score']\n",
    "data[scale_features] = ss.fit_transform(data[scale_features])\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Train Test Split (70-30 split)\n",
    "\n",
    "#data = data.drop('recruitment_channel', axis=1)\n",
    "X = data.drop('is_promoted', axis=1).values\n",
    "y = data['is_promoted'].values\n",
    "\n",
    "# Oversampling the data, define oversampling strategy\n",
    "#oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "# fit and apply the transform\n",
    "#X_over, y_over = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    ADAMAX = optimizers.Adamax(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999)\n",
    "    # Accuracy with ADAMAX at 0.944, loss at 0.153, F1 Score at 0.5222337125129266\n",
    "\n",
    "    ADAM = optimizers.Adam(lr = 0.01)\n",
    "    # Accuracy with ADAM at 0.945, loss at 0.146, F1 Score at 0.517427589592538\n",
    "\n",
    "    ADADELTA = optimizers.Adadelta(lr = 1.0, rho = 0.95)\n",
    "    # Accuracy with ADADELTA at , loss at 0.155, F1 Score at 0.506652474720596\n",
    "\n",
    "    ANN_model = Sequential()\n",
    "    ANN_model.add(Dense(64, input_shape = (X_train.shape[1],), activation = 'tanh'))\n",
    "    ANN_model.add(Dense(64, activation = 'tanh'))\n",
    "    ANN_model.add(Dense(64, activation = 'tanh'))\n",
    "    ANN_model.add(Dense(64, activation = 'tanh'))\n",
    "    ANN_model.add(Dense(16, activation = 'tanh'))\n",
    "    # Last layer to use sigmoid activation function (coz binary classification)\n",
    "    ANN_model.add(Dense(1, activation = 'sigmoid'))\n",
    "    ANN_model.compile(optimizer = ADAMAX, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return ANN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_split = 8\n",
    " \n",
    "for train_index,test_index in KFold(n_split).split(X):\n",
    "    X_train,X_test=X[train_index],X[test_index]\n",
    "    y_train,y_test=y[train_index],y[test_index]\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs = 20)\n",
    "    print('Model evaluation', model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ANN_history = ANmodel.fit(X_train, y_train, verbose = 0, epochs = 30)\n",
    "plot_loss_accuracy(ANN_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion matrix\n",
    "\n",
    "Y_Pred = ANN_model.predict(X_test)\n",
    "Cnf_matrix = confusion_matrix(y_test, Y_Pred.round())\n",
    "np.set_printoptions(precision = 2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(Cnf_matrix, classes=['Not Promoted','Promoted'],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, Y_Pred.round()).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "Accuracy = (tn+tp)*100/(tp+tn+fp+fn) \n",
    "print(\"Accuracy: {:0.2f}%\".format(Accuracy))\n",
    "print(\"Accuracy Score: {}\".format(accuracy_score(y_test, Y_Pred.round(), normalize = False)))\n",
    "\n",
    "#Precision \n",
    "Precision = tp/(tp+fp) \n",
    "print(\"Precision: {:0.2f}\".format(Precision))\n",
    "print(\"Precision Score: {}\".format(precision_score(y_test, Y_Pred.round(), pos_label = 1, average = 'binary')))\n",
    "\n",
    "#Recall \n",
    "Recall = tp/(tp+fn) \n",
    "print(\"Recall: {:0.2f}\".format(Recall))\n",
    "print(\"Recall Score: {}\".format(recall_score(y_test, Y_Pred.round(), pos_label = 1, average = 'binary')))\n",
    "\n",
    "#F1 Score\n",
    "f1 = (2*Precision*Recall)/(Precision + Recall)\n",
    "print(\"F1 Score {:0.2f}\".format(f1))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, Y_Pred.round(), pos_label = 1, average = 'binary')))\n",
    "\n",
    "#Specificity \n",
    "Specificity = tn/(tn+fp)\n",
    "print(\"Specificity: {:0.2f}\".format(Specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('C:/Users/ak19919/Downloads/ml_root/analytics vidya/hr_analytics/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "\n",
    "for column in ['education', 'previous_year_rating']:\n",
    "    test_data[column].fillna(test_data[column].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data['LOS_BC_0'] = stats.boxcox((1+data['length_of_service']), lmbda = 0)\n",
    "#test_data['LOS_BC_opt'] = stats.boxcox(test_data['length_of_service'], lmbda = opt_lambda)\n",
    "#test_data = test_data.drop('length_of_service', axis=1)\n",
    "#test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['joining_age'] = test_data_1['age'] - test_data['length_of_service']\n",
    "test_data['join_age_range'] = pd.qcut(test_data['joining_age'], q=quantile_list, labels=quantile_labels)\n",
    "test_data = test_data.drop('joining_age', axis=1)\n",
    "\n",
    "test_data['age_range'] = pd.qcut(test_data['age'], q = quantile_list, labels = quantile_labels)\n",
    "test_data = test_data.drop('age', axis = 1)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_T = pd.get_dummies(test_data[cat_features])\n",
    "test_data = test_data.drop(cat_features, axis = 1)\n",
    "test_data = pd.concat([test_data, df_cat_T], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features within range (0, 1)\n",
    "test_data[scale_features] = ss.fit_transform(test_data[scale_features])\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_id = test_data.drop('employee_id', axis = 1)\n",
    "test_predictions = ANN_model.predict(test_no_id)\n",
    "employee_ID = test_data['employee_id']\n",
    "submission_df_1 = pd.DataFrame({\n",
    "                  \"employee_id\": employee_ID, \n",
    "                  \"is_promoted\": test_predictions.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df_1.is_promoted = submission_df_1.is_promoted.round()\n",
    "submission_df_1.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ANN_history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
